{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¾®è°ƒå’Œéƒ¨ç½²å¯¹è¯æ¨¡å‹ChatGLM2-6B\n",
    "\n",
    "[ChatGLM2-6B](https://www.modelscope.cn/models/ZhipuAI/chatglm2-6b/summary)æ˜¯ä¸­è‹±æ–‡å¯¹è¯æ¨¡å‹[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) çš„ç¬¬äºŒä»£ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM2-6B å¼•å…¥äº†å¤šé¡¹å‡çº§ï¼ŒåŒ…æ‹¬æ›´å¼ºå¤§çš„æ€§èƒ½ã€æ›´é•¿çš„ä¸Šä¸‹æ–‡ã€æ›´é«˜æ•ˆçš„æ¨ç†ã€‚\n",
    "\n",
    "åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºï¼š\n",
    "\n",
    "- å°†ChatGLM2-6Béƒ¨ç½²åˆ°PAIåˆ›å»ºæ¨ç†æœåŠ¡ï¼ŒåŸºäºæ¨ç†æœåŠ¡APIå’ŒGradioå®ç°ä¸€ä¸ªç®€æ˜“å¯¹è¯æœºå™¨äººã€‚\n",
    "\n",
    "- åœ¨PAIå¯¹ChatGLM2-6Bè¿›è¡Œå¾®è°ƒè®­ç»ƒï¼Œå¹¶å°†å¾®è°ƒçš„æ¨¡å‹éƒ¨ç½²åˆ›å»ºæ¨ç†æœåŠ¡ã€‚\n",
    "\n",
    "\n",
    "## å‡†å¤‡å·¥ä½œ\n",
    "\n",
    "### å‰ææ¡ä»¶\n",
    "\n",
    "- å·²è·å–é˜¿é‡Œäº‘è´¦å·çš„é‰´æƒAccessKey IDå’ŒAccessKey Secretï¼Œè¯¦æƒ…è¯·å‚è§ï¼š[è·å–AccessKey](https://help.aliyun.com/document_detail/116401.html)ã€‚\n",
    "- å·²åˆ›å»ºæˆ–æ˜¯åŠ å…¥ä¸€ä¸ªPAI AIå·¥ä½œç©ºé—´ï¼Œè¯¦æƒ…è¯·å‚è§ï¼š[åˆ›å»ºå·¥ä½œç©ºé—´](https://help.aliyun.com/document_detail/326193.html)ã€‚\n",
    "- å·²åˆ›å»ºOSS Bucketï¼Œè¯¦æƒ…è¯·å‚è§ï¼š[æ§åˆ¶å°åˆ›å»ºå­˜å‚¨ç©ºé—´](https://help.aliyun.com/document_detail/31885.html)ã€‚\n",
    "\n",
    "\n",
    "### å®‰è£…å’Œé…ç½®PAI Python SDK\n",
    "\n",
    "æˆ‘ä»¬å°†ä½¿ç”¨PAIæä¾›çš„Python SDKï¼Œæäº¤è®­ç»ƒä½œä¸šï¼Œéƒ¨ç½²æ¨¡å‹ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…PAI Python SDKã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade alipai\n",
    "!python -m pip install gradio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SDKéœ€è¦é…ç½®è®¿é—®é˜¿é‡Œäº‘æœåŠ¡éœ€è¦çš„ AccessKeyï¼Œä»¥åŠå½“å‰ä½¿ç”¨çš„å·¥ä½œç©ºé—´å’ŒOSS Bucketã€‚åœ¨PAI Python SDKå®‰è£…ä¹‹åï¼Œé€šè¿‡åœ¨ **å‘½ä»¤è¡Œç»ˆç«¯** ä¸­æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼ŒæŒ‰ç…§å¼•å¯¼é…ç½®å¯†é’¥ï¼Œå·¥ä½œç©ºé—´ç­‰ä¿¡æ¯ã€‚\n",
    "\n",
    "\n",
    "```shell\n",
    "\n",
    "# ä»¥ä¸‹å‘½ä»¤ï¼Œè¯·åœ¨ å‘½ä»¤è¡Œç»ˆç«¯ ä¸­æ‰§è¡Œ.\n",
    "\n",
    "python -m pai.toolkit.config\n",
    "\n",
    "```\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç éªŒè¯å½“å‰çš„é…ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pai\n",
    "from pai.session import get_default_session\n",
    "\n",
    "print(pai.__version__)\n",
    "sess = get_default_session()\n",
    "\n",
    "assert sess.workspace_name is not None\n",
    "print(sess.workspace_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç›´æ¥éƒ¨ç½²ChatGLM2\n",
    "\n",
    "`ChatGLM2-6B`æ˜¯ä¸€ä¸ªå¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤ŸåŸºäºå†å²å¯¹è¯ä¿¡æ¯ï¼Œå’Œç”¨æˆ·çš„Promptè¾“å…¥ï¼Œè¿›è¡Œåé¦ˆã€‚é€šè¿‡HuggingFaceçš„transformersåº“ç”¨æˆ·å¯ä»¥ç›´æ¥ä½¿ç”¨`ChatGLM2-6B`æä¾›çš„å¯¹è¯èƒ½åŠ›ï¼Œç¤ºä¾‹å¦‚ä¸‹:\n",
    "\n",
    "```python\n",
    "\n",
    ">>> from transformers import AutoTokenizer, AutoModel\n",
    ">>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n",
    ">>> model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).half().cuda()\n",
    ">>> model = model.eval()\n",
    ">>> response, history = model.chat(tokenizer, \"ä½ å¥½\", history=[])\n",
    ">>> print(response)\n",
    "ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n",
    ">>> response, history = model.chat(tokenizer, \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\", history=history)\n",
    ">>> print(response)\n",
    "æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:\n",
    "\n",
    "1. åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚\n",
    "2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚\n",
    "3. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚\n",
    "4. é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚\n",
    "5. é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚\n",
    "6. å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”ã€‚\n",
    "\n",
    "å¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "ä»¥ä¸‹çš„æµç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†`ChatGLM2-6B`éƒ¨ç½²åˆ°PAIåˆ›å»ºä¸€ä¸ªæ¨ç†æœåŠ¡ï¼Œç„¶ååŸºäºæ¨ç†æœåŠ¡çš„APIï¼Œä½¿ç”¨Gradioåˆ›å»ºä¸€ä¸ªå¯¹è¯æœºå™¨äººã€‚"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### è·å–ChatGLM2æ¨¡å‹\n",
    "\n",
    "æ¨ç†æœåŠ¡å’Œè®­ç»ƒä½œä¸šä¸­éƒ½éœ€è¦åŠ è½½ä½¿ç”¨æ¨¡å‹ï¼ŒPAIåœ¨éƒ¨åˆ†regionä¸Šæä¾›æ¨¡å‹ç¼“å­˜ï¼Œæ”¯æŒç”¨æˆ·èƒ½å¤Ÿæ›´å¿«åœ°è·å–åˆ°ç›¸åº”çš„æ¨¡å‹ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç è·å–ç›¸åº”çš„æ¨¡å‹ï¼Œç„¶ååœ¨è®­ç»ƒä½œä¸šå’Œæ¨ç†æœåŠ¡ä¸­åŠ è½½ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pai.model import RegisteredModel\n",
    "\n",
    "m = RegisteredModel(\n",
    "    \"THUDM/chatglm2-6b\",\n",
    "    model_provider=\"huggingface\",\n",
    ")\n",
    "\n",
    "model_uri = m.model_data\n",
    "print(model_uri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ›å»ºæ¨ç†æœåŠ¡\n",
    "\n",
    "PAI-EASæ˜¯é˜¿é‡Œäº‘PAIæä¾›æ¨¡å‹åœ¨çº¿æœåŠ¡å¹³å°ï¼Œæ”¯æŒç”¨æˆ·ä¸€é”®éƒ¨ç½²æ¨ç†æœåŠ¡æˆ–æ˜¯AIWebåº”ç”¨ï¼Œæ”¯æŒå¼‚æ„èµ„æºï¼Œå¼¹æ€§æ‰©ç¼©å®¹ã€‚PAI-EASæ”¯æŒä½¿ç”¨é•œåƒçš„æ–¹å¼éƒ¨ç½²æ¨¡å‹ï¼Œä»¥ä¸‹çš„æµç¨‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨PAIæä¾›çš„PyTorchæ¨ç†é•œåƒï¼Œå°†ä»¥ä¸Šçš„æ¨¡å‹éƒ¨ç½²ä¸ºæ¨ç†æœåŠ¡ã€‚\n",
    "\n",
    "\n",
    "åœ¨éƒ¨ç½²æ¨ç†æœåŠ¡ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡ç›¸åº”çš„æ¨ç†æœåŠ¡ç¨‹åºï¼Œä»–è´Ÿè´£åŠ è½½æ¨¡å‹ï¼Œæä¾›å¯¹åº”çš„HTTP APIæœåŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p server_src"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®Œæ•´çš„æ¨ç†æœåŠ¡ä»£ç å¦‚ä¸‹:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile server_src/run.py\n",
    "# source: https://github.com/THUDM/ChatGLM-6B/blob/main/api.py\n",
    "\n",
    "import os\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import uvicorn, json, datetime\n",
    "import torch\n",
    "\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "# é»˜è®¤çš„æ¨¡å‹ä¿å­˜è·¯å¾„\n",
    "chatglm_model_path = \"/eas/workspace/model/\"\n",
    "# ptuning checkpointsä¿å­˜è·¯å¾„\n",
    "ptuning_checkpoint = \"/ml/ptuning_checkpoints/\"\n",
    "pre_seq_len = 128\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    global model, tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(chatglm_model_path, trust_remote_code=True)\n",
    "\n",
    "    if os.path.exists(ptuning_checkpoint):\n",
    "        # P-tuning v2\n",
    "        print(f\"Loading model/ptuning_checkpoint weight...\")\n",
    "        config = AutoConfig.from_pretrained(chatglm_model_path, trust_remote_code=True)\n",
    "        config.pre_seq_len = pre_seq_len\n",
    "        config.prefix_projection = False\n",
    "\n",
    "        model = AutoModel.from_pretrained(chatglm_model_path, config=config, trust_remote_code=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(chatglm_model_path, trust_remote_code=True)\n",
    "        prefix_state_dict = torch.load(os.path.join(ptuning_checkpoint, \"pytorch_model.bin\"))\n",
    "        new_prefix_state_dict = {}\n",
    "        for k, v in prefix_state_dict.items():\n",
    "            if k.startswith(\"transformer.prefix_encoder.\"):\n",
    "                new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
    "        model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
    "\n",
    "        model = model.half().cuda()\n",
    "        model.transformer.prefix_encoder.float().cuda()\n",
    "        model.eval()\n",
    "    else:\n",
    "        print(f\"Loading model weight...\")\n",
    "        model = AutoModel.from_pretrained(chatglm_model_path, trust_remote_code=True)\n",
    "        model.half().cuda()\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "\n",
    "@app.post(\"/\")\n",
    "async def create_item(request: Request):\n",
    "    global model, tokenizer\n",
    "    json_post_raw = await request.json()\n",
    "    json_post = json.dumps(json_post_raw)\n",
    "    json_post_list = json.loads(json_post)\n",
    "    prompt = json_post_list.get('prompt')\n",
    "    history = json_post_list.get('history')\n",
    "    max_length = json_post_list.get('max_length')\n",
    "    top_p = json_post_list.get('top_p')\n",
    "    temperature = json_post_list.get('temperature')\n",
    "    response, history = model.chat(tokenizer,\n",
    "                                   prompt,\n",
    "                                   history=history,\n",
    "                                   max_length=max_length if max_length else 2048,\n",
    "                                   top_p=top_p if top_p else 0.7,\n",
    "                                   temperature=temperature if temperature else 0.95)\n",
    "    now = datetime.datetime.now()\n",
    "    time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    answer = {\n",
    "        \"response\": response,\n",
    "        \"history\": history,\n",
    "        \"status\": 200,\n",
    "        \"time\": time\n",
    "    }\n",
    "    log = \"[\" + time + \"] \" + '\", prompt:\"' + prompt + '\", response:\"' + repr(response) + '\"'\n",
    "    print(log)\n",
    "    return answer\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    load_model()\n",
    "    uvicorn.run(app, host='0.0.0.0', port=8000, workers=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å°†ä½¿ç”¨PyTorché•œåƒè¿è¡Œç›¸åº”çš„æ¨ç†æœåŠ¡ï¼Œåœ¨å¯åŠ¨æœåŠ¡ä¹‹å‰éœ€è¦å®‰è£…æ¨¡å‹ä¾èµ–çš„ç›¸å…³çš„ä¾èµ–ã€‚æˆ‘ä»¬å¯ä»¥åœ¨`server_src`ä¸‹å‡†å¤‡ä¾èµ–çš„`requirements.txt`ï¼Œå¯¹åº”çš„`requirements.txt`ä¼šåœ¨æ¨ç†æœåŠ¡å¯åŠ¨ä¹‹å‰è¢«å®‰è£…åˆ°ç¯å¢ƒä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile server_src/requirements.txt\n",
    "\n",
    "# æ¨¡å‹éœ€è¦çš„ä¾èµ–\n",
    "transformers==4.30.2\n",
    "accelerate\n",
    "icetk\n",
    "cpm_kernels\n",
    "\n",
    "torch>=2.0,<2.1\n",
    "gradio\n",
    "mdtex2html\n",
    "sentencepiece\n",
    "accelerate\n",
    "\n",
    "# æ¨ç†æœåŠ¡Serverçš„ä¾èµ–\n",
    "fastapi\n",
    "uvicorn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "åŸºäºä»¥ä¸Šçš„æ¨ç†æœåŠ¡ç¨‹åºï¼Œæˆ‘ä»¬å°†ä½¿ç”¨PyTorché•œåƒå’ŒOSSä¸Šçš„æ¨¡å‹åœ¨PAIåˆ›å»ºä¸€ä¸ªæ¨ç†æœåŠ¡ï¼Œä»£ç å¦‚ä¸‹ã€‚\n",
    "\n",
    "> å¯¹äºå¦‚ä½•ä½¿ç”¨SDKåˆ›å»ºæ¨ç†æœåŠ¡çš„è¯¦ç»†ä»‹ç»ï¼Œè¯·è§æ–‡æ¡£ï¼š[åˆ›å»ºæ¨ç†æœåŠ¡](https://help.aliyun.com/document_detail/2261532.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pai.model import container_serving_spec, Model\n",
    "from pai.image import retrieve, ImageScope\n",
    "from pai.common.utils import random_str\n",
    "\n",
    "\n",
    "# InferenceSpecç”¨äºæè¿°å¦‚ä½•åˆ›å»ºæ¨ç†æœåŠ¡\n",
    "infer_spec = container_serving_spec(\n",
    "    # ä½¿ç”¨PAIæä¾›çš„æœ€æ–°PyTorchçš„æ¨ç†é•œåƒ\n",
    "    image_uri=retrieve(\n",
    "        \"PyTorch\",\n",
    "        \"latest\",\n",
    "        accelerator_type=\"GPU\",\n",
    "        image_scope=ImageScope.INFERENCE,\n",
    "    ),\n",
    "    source_dir=\"./server_src\",\n",
    "    command=\"python run.py\",\n",
    ")\n",
    "\n",
    "m = Model(\n",
    "    # æ¨¡å‹çš„OSSè·¯å¾„ï¼Œé»˜è®¤æ¨¡å‹ä¼šé€šè¿‡æŒ‚è½½çš„æ–¹å¼æŒ‚è½½åˆ°`/eas/workspace/model/`è·¯å¾„ä¸‹ã€‚\n",
    "    model_data=model_uri,\n",
    "    inference_spec=infer_spec,\n",
    ")\n",
    "\n",
    "\n",
    "# éƒ¨ç½²æ¨¡å‹ï¼Œåˆ›å»ºæ¨ç†æœåŠ¡.\n",
    "p = m.deploy(\n",
    "    service_name=\"chatglm_demo_{}\".format(random_str(6)),\n",
    "    instance_type=\"ecs.gn6i-c8g1.2xlarge\",  # 8vCPU 31GB NVIDIA T4Ã—1(GPU Mem 16GB)\n",
    "    options={\n",
    "        # é…ç½®EAS RPCæ¡†æ¶çš„è¶…æ—¶æ—¶é—´, å•ä½ä¸ºæ¯«ç§’\n",
    "        \"metadata.rpc.keepalive\": 20000,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(p.service_name)\n",
    "print(p.service_status)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`m.deploy`è¿”å›ä¸€ä¸ªPredictorå¯¹è±¡ï¼Œå¯ä»¥ç”¨äºå‘åˆ›å»ºçš„æ¨ç†æœåŠ¡ç¨‹åºå‘é€é¢„æµ‹è¯·æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pai.predictor import RawResponse\n",
    "\n",
    "resp: RawResponse = p.raw_predict(\n",
    "    {\n",
    "        \"prompt\": \"ä½ å¥½\",\n",
    "    }\n",
    ")\n",
    "print(resp.json()[\"response\"])\n",
    "\n",
    "\n",
    "resp = p.raw_predict(\n",
    "    {\n",
    "        \"prompt\": \"æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ\",\n",
    "        \"history\": resp.json()[\"history\"],\n",
    "    },\n",
    "    timeout=20,\n",
    ")\n",
    "print(resp.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŸºäºä»¥ä¸Šçš„æ¨ç†æœåŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Gradioåˆ›å»ºä¸€ä¸ªç®€å•çš„å¯¹è¯æœºå™¨äººdemoã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "    submit = gr.Button(\"Submit\")\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "\n",
    "        print(f\"Message: {message}\")\n",
    "        print(f\"ChatHistory: {chat_history}\")\n",
    "        resp = p.raw_predict(\n",
    "            {\n",
    "                \"prompt\": message,\n",
    "                \"history\": chat_history,\n",
    "            }\n",
    "        ).json()\n",
    "        print(f\"Response: {resp['response']}\")\n",
    "\n",
    "        chat_history.append((message, resp[\"response\"]))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    submit.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€šè¿‡ä»¥ä¸Šåˆ›å»ºçš„Gradioåº”ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é¡µé¢ä¸Šä¸éƒ¨ç½²çš„ChatGLMæ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "åœ¨æµ‹è¯•å®Œæˆä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹çš„ä»£ç åˆ é™¤æ¨ç†æœåŠ¡ï¼Œé‡Šæ”¾èµ„æºã€‚\n",
    "\n",
    "> è¯·æ³¨æ„ï¼Œåˆ é™¤åœ¨çº¿æ¨ç†æœåŠ¡ä¹‹åï¼Œå¯¹åº”çš„Gradioçš„åº”ç”¨å°†æ— æ³•ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.delete_service()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¾®è°ƒChatGLM2-6B\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥ä½¿ç”¨é¢†åŸŸæ•°æ®å¯¹ChatGLMè¿›è¡Œå¾®è°ƒï¼Œä»è€Œä½¿å¾—æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸå’Œä»»åŠ¡ä¸‹æœ‰æ›´å¥½çš„è¡¨ç°ã€‚ChatGLMå›¢é˜Ÿæä¾›äº†ä½¿ç”¨[P-Tuning v2](https://github.com/THUDM/P-tuning-v2)æ–¹å¼å¯¹æ¨¡å‹è¿›è¡Œ[å¾®è°ƒçš„æ–¹æ¡ˆ](https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning)ï¼Œæˆ‘ä»¬å°†åŸºäºæ­¤æ–¹æ¡ˆå±•ç¤ºå¦‚ä½•å°†å¾®è°ƒè®­ç»ƒä½œä¸šæäº¤åˆ°PAIçš„è®­ç»ƒæœåŠ¡æ‰§è¡Œã€‚\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‡†å¤‡è®­ç»ƒæ•°æ®é›†\n",
    "\n",
    "æˆ‘ä»¬å°†ä½¿ç”¨äº†[å¹¿å‘Šç”Ÿæˆæ•°æ®é›†](https://aclanthology.org/D19-1321.pdf)ï¼Œå¯¹ChatGLMè¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬é¦–å…ˆéœ€è¦å‡†å¤‡æ•°æ®åˆ°OSSï¼Œä¾›åç»­å¾®è°ƒè®­ç»ƒä½œä¸šä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pai.common.oss_utils import download, OssUriObj, upload\n",
    "import zipfile\n",
    "\n",
    "# ä¸‹è½½æ•°æ®\n",
    "data = download(\n",
    "    # å½“å‰çš„æ•°æ®é›†åœ¨ä¸Šæµ·regionï¼Œè·¨regionä¸‹è½½ï¼Œæˆ‘ä»¬éœ€è¦ä¼ é€’å¯¹åº”OSS Bucketæ‰€åœ¨Endpoint.\n",
    "    OssUriObj(\n",
    "        \"oss://atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com/release/tutorials/chatGLM/AdvertiseGen_Simple.zip\"\n",
    "    ),\n",
    "    local_path=\"./\",\n",
    ")\n",
    "\n",
    "# è§£å‹ç¼©æ•°æ®\n",
    "with zipfile.ZipFile(data, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./train_data/\")\n",
    "\n",
    "# ä¸Šä¼ æ•°æ®åˆ°OSS\n",
    "train_data = \"./train_data/AdvertiseGen_Simple/\"\n",
    "train_data_uri = upload(\n",
    "    \"./train_data/AdvertiseGen_Simple/\", oss_path=\"chatglm_demo/data/advertisegen/\"\n",
    ")\n",
    "print(train_data_uri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç›¸åº”çš„æ•°æ®é›†æ•°æ®æ ¼å¼å¦‚ä¸‹:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 ./train_data/AdvertiseGen_Simple/train.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å‡†å¤‡å¾®è°ƒè®­ç»ƒä½œä¸šè„šæœ¬\n",
    "\n",
    "ChatGLMçš„å®˜æ–¹æä¾›[å¾®è°ƒè®­ç»ƒè„šæœ¬](https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning)ï¼Œæ”¯æŒä½¿ç”¨P-Tuning v2çš„æ–¹å¼å¯¹ChatGLMæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å°†åŸºäºç›¸åº”çš„å¾®è°ƒè®­ç»ƒè„šæœ¬ï¼Œä¿®æ”¹è®­ç»ƒä½œä¸šçš„æ‹‰èµ·Shellè„šæœ¬(`train.sh`)ï¼Œç„¶åä½¿ç”¨PAI Python SDKå°†å¾®è°ƒè®­ç»ƒä½œä¸šæäº¤åˆ°PAIæ‰§è¡Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½ChatGLMä»£ç \n",
    "!git clone https://github.com/THUDM/ChatGLM2-6B.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "å½“è®­ç»ƒä½œä¸šæäº¤åˆ°PAIæ‰§è¡Œæ—¶ï¼Œéœ€è¦æŒ‰ä¸€å®šè§„èŒƒè¯»å–è¾“å…¥æ•°æ®ï¼Œä»¥åŠå°†éœ€è¦ä¿å­˜çš„æ¨¡å‹å†™å‡ºåˆ°æŒ‡å®šè·¯å¾„ä¸‹ï¼Œæ›´åŠ å…·ä½“ä»‹ç»è¯·è§æ–‡æ¡£ï¼š[æäº¤è®­ç»ƒä½œä¸š](https://help.aliyun.com/document_detail/2261505.html)ã€‚\n",
    "\n",
    "ä¿®æ”¹åçš„è®­ç»ƒä½œä¸šæ‹‰èµ·è„šæœ¬å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile ChatGLM2-6B/ptuning/train.sh\n",
    "\n",
    "PRE_SEQ_LEN=128\n",
    "LR=2e-2\n",
    "NUM_GPUS=`nvidia-smi --list-gpus | wc -l`\n",
    "\n",
    "torchrun --standalone --nnodes=1 --nproc-per-node=$NUM_GPUS main.py \\\n",
    "    --do_train \\\n",
    "    --train_file /ml/input/data/train/train.json \\\n",
    "    --validation_file /ml/input/data/train/dev.json \\\n",
    "    --preprocessing_num_workers 10 \\\n",
    "    --prompt_column content \\\n",
    "    --response_column summary \\\n",
    "    --overwrite_cache \\\n",
    "    --model_name_or_path /ml/input/data/model \\\n",
    "    --output_dir /ml/output/model/ \\\n",
    "    --overwrite_output_dir \\\n",
    "    --max_source_length 64 \\\n",
    "    --max_target_length 128 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 32 \\\n",
    "    --predict_with_generate \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --save_strategy epoch \\\n",
    "    --learning_rate $LR \\\n",
    "    --pre_seq_len $PRE_SEQ_LEN \\\n",
    "    --quantization_bit 4\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œæˆ‘ä»¬å°†ä½¿ç”¨PAIæä¾›çš„PyTorch GPUè®­ç»ƒé•œåƒè¿è¡Œè®­ç»ƒä½œä¸šï¼Œéœ€è¦å®‰è£…éƒ¨åˆ†ç¬¬ä¸‰æ–¹ä¾èµ–åŒ…ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡æä¾›`requirements.txt`çš„æ–¹å¼æä¾›ï¼Œç›¸åº”çš„ä¾èµ–ä¼šåœ¨è®­ç»ƒä½œä¸šæ‰§è¡Œå‰è¢«å®‰è£…åˆ°ç¯å¢ƒä¸­\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile ChatGLM2-6B/ptuning/requirements.txt\n",
    "# æ¨¡å‹éœ€è¦çš„ä¾èµ–\n",
    "transformers==4.30.2\n",
    "accelerate\n",
    "icetk\n",
    "cpm_kernels\n",
    "\n",
    "torch>=2.0,<2.1\n",
    "sentencepiece\n",
    "accelerate\n",
    "\n",
    "rouge_chinese\n",
    "nltk\n",
    "jieba\n",
    "datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æäº¤è®­ç»ƒä½œä¸š\n",
    "\n",
    "æˆ‘ä»¬å°†é€šè¿‡PAI Python SDKï¼Œå°†ä»¥ä¸Šçš„è®­ç»ƒä½œä¸šæäº¤åˆ°PAIæ‰§è¡Œã€‚SDKåœ¨æäº¤è®­ç»ƒä½œä¸šä¹‹åï¼Œä¼šæ‰“å°è®­ç»ƒä½œä¸šçš„é“¾æ¥ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡å¯¹åº”çš„é“¾æ¥æŸ¥çœ‹ä½œä¸šçš„æ‰§è¡Œè¯¦æƒ…ï¼Œè¾“å‡ºæ—¥å¿—ã€‚\n",
    "\n",
    "> Noteï¼šæŒ‰å½“å‰ç¤ºä¾‹æ•™ç¨‹ä½¿ç”¨çš„è®­ç»ƒé…ç½®ã€æ•°æ®é›†å’Œæœºå™¨è§„æ ¼ï¼Œè®­ç»ƒä½œä¸šè¿è¡Œçº¦10åˆ†é’Ÿå·¦å³ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pai.estimator import Estimator\n",
    "from pai.image import retrieve\n",
    "\n",
    "# ä½¿ç”¨PAIæä¾›çš„æœ€æ–°çš„PyTorchæ¨ç†é•œåƒ\n",
    "image_uri = retrieve(\n",
    "    \"PyTorch\",\n",
    "    \"latest\",\n",
    "    accelerator_type=\"GPU\",\n",
    ").image_uri\n",
    "\n",
    "\n",
    "est = Estimator(\n",
    "    command=\"bash train.sh\",  # å¯åŠ¨å‘½ä»¤\n",
    "    source_dir=\"./ChatGLM2-6B/ptuning\",  # è®­ç»ƒä»£ç ç›®å½•.\n",
    "    image_uri=image_uri,  # è®­ç»ƒé•œåƒ\n",
    "    instance_type=\"ecs.gn6e-c12g1.3xlarge\",  # ä½¿ç”¨çš„æœºå™¨è§„æ ¼ç¤ºä¾‹ï¼ŒV100(32G)\n",
    "    base_job_name=\"chatglm2_finetune_\",\n",
    ")\n",
    "\n",
    "\n",
    "# æäº¤è®­ç»ƒä½œä¸š\n",
    "est.fit(\n",
    "    inputs={\n",
    "        \"model\": model_uri,\n",
    "        \"train\": train_data_uri,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é»˜è®¤`estimator.fit`ä¼šç­‰å¾…åˆ°ä½œä¸šæ‰§è¡Œå®Œæˆã€‚ä½œä¸šæ‰§è¡ŒæˆåŠŸä¹‹åï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡`est.model_data()`è·å–è¾“å‡ºæ¨¡å‹åœ¨OSSä¸Šçš„è·¯å¾„åœ°å€ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(est.model_data())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”¨æˆ·å¯ä»¥é€šè¿‡`ossutil`æˆ–æ˜¯SDKæä¾›çš„ä¾¿åˆ©æ–¹æ³•å°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°:\n",
    "\n",
    "```python\n",
    "from pai.common.oss_util import download\n",
    "\n",
    "\n",
    "# ä½¿ç”¨SDKçš„ä¾¿åˆ©æ–¹æ³•ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°.\n",
    "download(\n",
    "\toss_path=est.model_data(),\n",
    "\tlocal_path=\"./output_model\",\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### éƒ¨ç½²å¾®è°ƒä¹‹åçš„æ¨¡å‹\n",
    "\n",
    "å¾®è°ƒè®­ç»ƒä¹‹åè·å¾—çš„`checkpoints`ï¼Œéœ€è¦å’ŒåŸå§‹çš„æ¨¡å‹é…åˆä¸€èµ·ä½¿ç”¨ã€‚æˆ‘ä»¬éœ€è¦é€šè¿‡ä»¥ä¸‹ä»£ç è·å¾—å¯¹åº”çš„checkpointè·¯å¾„.\n",
    "\n",
    "> ç”¨æˆ·é€šè¿‡ä¿®æ”¹å¾®è°ƒè®­ç»ƒçš„ä»£ç ï¼Œä½¿ç”¨`Trainer.save_model()`æ˜¾å¼çš„ä¿å­˜ç›¸åº”çš„checkpointsï¼Œåˆ™å¯ä»¥ç›´æ¥é€šè¿‡`estimator.model_data()`ä¸‹è·å¾—ç›¸åº”çš„checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ä»¥ä¸Šçš„è®­ç»ƒä½œä¸šè¶…å‚è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬è®¾ç½®`epochs=2`, checkpointsä¿å­˜çš„ç­–ç•¥æ˜¯`æ¯ä¸€ä¸ªepochsä¿å­˜`ã€‚\n",
    "# é»˜è®¤æœ€åä¸€ä¸ªcheckpointä¼šè¢«ä¿å­˜åˆ°`{output_dir}/checkpoint-2`è·¯å¾„ä¸‹.\n",
    "# é€šè¿‡ä»¥ä¸‹è·¯å¾„ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—æ¨¡å‹è®­ç»ƒè·å¾—çš„æœ€åä¸€ä¸ªcheckpointçš„OSSè·¯å¾„.\n",
    "\n",
    "checkpoint_uri = os.path.join(est.model_data(), \"checkpoint-10/\")\n",
    "print(checkpoint_uri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å°†å¤ç”¨ChatGLM2éƒ¨ç½²çš„æ¨ç†æœåŠ¡ç¨‹åºåˆ›å»ºæ¨ç†æœåŠ¡ã€‚ä¸ç›´æ¥éƒ¨ç½²ChatGLM2çš„ä¸åŒç‚¹åœ¨äºæˆ‘ä»¬è¿˜éœ€è¦æä¾›å¾®è°ƒä¹‹åè·å¾—çš„checkpointsã€‚\n",
    "\n",
    "é€šè¿‡`InferenceSpec.mount` APIï¼Œæˆ‘ä»¬å¯ä»¥å°†ç›¸åº”çš„OSSæ¨¡å‹è·¯å¾„æŒ‚è½½åˆ°æœåŠ¡å®¹å™¨ä¸­ï¼Œä¾›æ¨ç†æœåŠ¡ç¨‹åºä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pai.model import container_serving_spec, Model\n",
    "from pai.image import retrieve, ImageScope\n",
    "\n",
    "\n",
    "# InferenceSpecç”¨äºæè¿°å¦‚ä½•åˆ›å»ºæ¨ç†æœåŠ¡\n",
    "infer_spec = container_serving_spec(\n",
    "    image_uri=retrieve(  # ä½¿ç”¨PAIæä¾›çš„æœ€æ–°PyTorchçš„æ¨ç†é•œåƒ\n",
    "        \"PyTorch\",\n",
    "        \"latest\",\n",
    "        accelerator_type=\"GPU\",\n",
    "        image_scope=ImageScope.INFERENCE,\n",
    "    ),\n",
    "    source_dir=\"./server_src\",  # ä»£ç ç›®å½•\n",
    "    command=\"python run.py\",  # å¯åŠ¨å‘½ä»¤\n",
    ")\n",
    "\n",
    "\n",
    "# å°†ç›¸åº”çš„checkpointsæŒ‚è½½åˆ°æœåŠ¡ä¸­ï¼Œæ¨ç†æœåŠ¡çš„ç¨‹åºé€šè¿‡æ£€æŸ¥ç›®å½•(/ml/ptuning_checkpoints/)æ˜¯å¦å­˜åœ¨åŠ è½½checkpoints\n",
    "infer_spec.mount(checkpoint_uri, \"/ml/ptuning_checkpoints\")\n",
    "print(infer_spec.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pai.common.utils import random_str\n",
    "\n",
    "m = Model(\n",
    "    model_data=model_uri,\n",
    "    inference_spec=infer_spec,\n",
    ")\n",
    "\n",
    "# éƒ¨ç½²æ¨¡å‹\n",
    "p = m.deploy(\n",
    "    service_name=\"chatglm_ft_{}\".format(random_str(6)),\n",
    "    instance_type=\"ecs.gn6i-c16g1.4xlarge\",  # 1 * T4\n",
    "    options={\n",
    "        # é…ç½®EAS RPCæ¡†æ¶çš„è¶…æ—¶æ—¶é—´, å•ä½ä¸ºæ¯«ç§’\n",
    "        \"metadata.rpc.keepalive\": 20000,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‘æ¨ç†æœåŠ¡å‘é€è¯·æ±‚ï¼Œæµ‹è¯•æ¨ç†æœåŠ¡æ˜¯å¦æ­£å¸¸å¯åŠ¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = p.raw_predict(\n",
    "    {\n",
    "        \"prompt\": \"ä½ å¥½\",\n",
    "    },\n",
    ")\n",
    "print(resp.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŸºäºä»¥ä¸Šå¾®è°ƒåæ¨¡å‹çš„æ¨ç†æœåŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Gradioåˆ›å»ºä¸€ä¸ªæ–°çš„æœºå™¨äººã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "    submit = gr.Button(\"Submit\")\n",
    "\n",
    "    def respond(message, chat_history):\n",
    "\n",
    "        print(f\"Message: {message}\")\n",
    "        print(f\"ChatHistory: {chat_history}\")\n",
    "        resp = p.raw_predict(\n",
    "            {\n",
    "                \"prompt\": message,\n",
    "                \"history\": chat_history,\n",
    "            }\n",
    "        ).json()\n",
    "        print(f\"Response: {resp['response']}\")\n",
    "\n",
    "        chat_history.append((message, resp[\"response\"]))\n",
    "        return \"\", chat_history\n",
    "\n",
    "    submit.click(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨æµ‹è¯•å®Œæˆä¹‹åï¼Œå¯ä»¥é€šè¿‡`p.delete_service()`åˆ é™¤æœåŠ¡ï¼Œé‡Šæ”¾èµ„æºã€‚\n",
    "\n",
    "> è¯·æ³¨æ„ï¼Œåˆ é™¤åœ¨çº¿æ¨ç†æœåŠ¡ä¹‹åï¼Œå¯¹åº”çš„Gradioçš„åº”ç”¨å°†æ— æ³•ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.delete_service()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
